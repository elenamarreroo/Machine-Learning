{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PRÁCTICA 4:  Machine Learning - Hackathon**\n",
        "### Universitat de València, Escola Tecnica Superior d'Enginyeria\n",
        "### Elena Marrero Castellano | 3ª curso del Grado Ciencia de Datos"
      ],
      "metadata": {
        "id": "FK9WNXCYsUWT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr6ssFaJ2rYG"
      },
      "source": [
        "Descargamos las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sShT3Co2rYG"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import decomposition\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import preprocessing, decomposition \n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpT184kg2rYH"
      },
      "source": [
        "Descargamos nuestros datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvIw4hMu2rYH"
      },
      "outputs": [],
      "source": [
        "Xtrain = pd.read_csv(\"X_train.csv.gz\") \n",
        "ytrain = pd.read_csv(\"y_train.csv.gz\") \n",
        "Xtest = pd.read_csv(\"X_test.csv.gz\")\n",
        "# Convertimos de dataframe a arrays numpy\n",
        "Xtrain = Xtrain.values[:, 1:]\n",
        "ytrain = ytrain.values[:, 1:]\n",
        "Xtest = Xtest.values[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEES7x1q2rYI"
      },
      "source": [
        "**Primer paso**, realizar el preprocesado de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4zBp_D32rYI"
      },
      "outputs": [],
      "source": [
        "# Aplicamos la escala de MinMax a nuestros datos, Este estimador escala y traduce cada característica individualmente \n",
        "# de modo que esté en el rango dado en el conjunto de entrenamiento, por ejemplo, entre cero y uno.\n",
        "scaler = MinMaxScaler()\n",
        "scaler2 = MinMaxScaler()\n",
        "\n",
        "# Ajuste solo en el set de entrenamiento.\n",
        "scaler.fit(Xtrain)\n",
        "scaler2.fit(ytrain)\n",
        "\n",
        "# Aplicamos la transformación tanto al conjunto de train como al conjunto de test.\n",
        "X_train_scaled = scaler.transform(Xtrain)\n",
        "X_test_scaled = scaler.transform(Xtest)\n",
        "y_train_scaled = scaler2.transform(ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss8Gk9tL2rYJ"
      },
      "outputs": [],
      "source": [
        "# # 90 características son muchas características, por lo que intentamos reducir la dimensionalidad implementando PCA.\n",
        "# # Primero normalizamos sus datos usando scikit StandardScaler. Este es un paso necesario para pca:\n",
        "# # Primero dividimos los datos (en nuestro caso ya está hecho)\n",
        "# # Segundo, creamos la función para estandarizar y la entrenamos\n",
        "# scaler = preprocessing.StandardScaler()\n",
        "# scaler2 = preprocessing.StandardScaler()\n",
        "\n",
        "# # Tercero, entrenamos con los datos de X_train\n",
        "# scaler.fit(X_train_scaled) \n",
        "# scaler2.fit(y_train_scaled) \n",
        "\n",
        "# # Cuarto, estandarizamos aplicando la función a los datos que queremos estandarizar\n",
        "# X_train_estandarizado = scaler.transform(X_train_scaled)  \n",
        "# X_test_estandarizado = scaler.transform(X_test_scaled) \n",
        "# y_train_estandarizado = scaler2.transform(y_train_scaled) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSL5WC-E2rYJ",
        "outputId": "2a3c290b-b5b1-4005-88c0-e1f2b7c12ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.78320419, 0.48413238, 0.56832644, ..., 0.42161882, 0.4980812 ,\n",
              "        0.31204558],\n",
              "       [0.65074972, 0.54905976, 0.52135006, ..., 0.44243678, 0.51418811,\n",
              "        0.31348747],\n",
              "       [0.48489469, 0.47768308, 0.40661454, ..., 0.44116637, 0.50206017,\n",
              "        0.3098551 ],\n",
              "       ...,\n",
              "       [0.76135618, 0.55887676, 0.52385109, ..., 0.41233513, 0.50486791,\n",
              "        0.38347757],\n",
              "       [0.82004854, 0.58435993, 0.5161119 , ..., 0.42633983, 0.49607448,\n",
              "        0.31501065],\n",
              "       [0.7320315 , 0.57311822, 0.50345402, ..., 0.42404866, 0.493367  ,\n",
              "        0.32243068]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INY_KeDG2rYK"
      },
      "source": [
        "**Segundo paso**, extracción de características. Si queremos hacer algo básico podríamos simplemnte mirar la PCA o podemos probar los diferentes números de dimensiones para el PCA. O lo que sería más interesante, investigat algunas librerías utilizadas del sklearn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6d-AgtQ2rYL"
      },
      "source": [
        "LinearRegression() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lilMldG2rYL",
        "outputId": "76d4feba-431d-4bae-adac-6ba3ddbbc203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 0.2354512029158744 RMSE 9.626536091949268 MAE 6.84273834540552\n"
          ]
        }
      ],
      "source": [
        "# Necesitamos un modelo para nuestros experimentos. Usaremos un modelo sencillo de regresión lineal\n",
        "lr = LinearRegression() \n",
        "\n",
        "# Vamos a entrenar y testear nuestro modelo usando solo el conjunto de entrenamiento.\n",
        "lr.fit(Xtrain, ytrain) \n",
        "\n",
        "# Por defecto `score` calcula el índice de correlación de Pearson al cuadrado\n",
        "R2 = lr.score(Xtrain, ytrain)\n",
        "\n",
        "def rmse(y, yp):\n",
        "    return np.sqrt(np.sum((y-yp)**2) / y.shape[0])\n",
        "\n",
        "def mae(y, yp):\n",
        "    return np.sum(np.abs((y-yp)) / y.shape[0])\n",
        "\n",
        "yp = lr.predict(Xtrain) # Predecimos utilizando el conjunto de entrenamiento\n",
        "print('R2', R2, 'RMSE', rmse(ytrain, yp), 'MAE', mae(ytrain, yp))\n",
        "\n",
        "# Pasamos a DaraFrame\n",
        "y_hat = pd.DataFrame(yp)\n",
        "# Guardamos .csv (lo que hay que subir)\n",
        "y_hat.to_csv('y_hat.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Znqn-A2rYL"
      },
      "source": [
        "PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcd-ElBk2rYL",
        "outputId": "7a849c72-9400-4cd3-c56d-e2ce158df8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En este caso, el 90% de la variación equivale a 39  componentes principales\n"
          ]
        }
      ],
      "source": [
        "# Nuestros valores están normalizados, por lo que ahora aplicamos PCA\n",
        "# Crea una instancia del modelo\n",
        "pca = PCA(.90)\n",
        "# PCA (.90) significa que scikit-learn elige el número mínimo de componentes principales de manera que se retiene el \n",
        "# 90% de la varianza.\n",
        "pca.fit(X_train_scaled) \n",
        "# Imprime el número de componentes generados\n",
        "print(\"En este caso, el 90% de la variación equivale a\",pca.n_components_,\" componentes principales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGm_-sDr2rYM"
      },
      "outputs": [],
      "source": [
        "# Sabiendo que tenemos 39 componentes principales:\n",
        "model_pca = decomposition.PCA(n_components=39)\n",
        "model_pca.fit(X_train_scaled) \n",
        "Xpca = model_pca.transform(X_train_scaled) \n",
        "ypca = y_train_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GBM8LMc2rYM",
        "outputId": "6189b510-4dda-4b0e-c7e5-2d201f952eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000, 39)\n",
            "(200000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(Xpca.shape)\n",
        "print(ypca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPA-o5c22rYM"
      },
      "outputs": [],
      "source": [
        "Xpca1 = Xpca[0:2000]\n",
        "ypca1 = ypca[0:2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "K0yNFdWd2rYM",
        "outputId": "01dc852b-1168-4969-9663-16adaff51b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2000, 39)\n",
            "(2000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(Xpca1.shape)\n",
        "print(ypca1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNR7lF4c2rYN",
        "outputId": "044f298f-ddfd-4d2b-e756-107b913db52c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 897 tasks      | elapsed:    7.4s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    8.2s finished\n",
            "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0,\n",
              "                                 fit_intercept=True, intercept_scaling=1.0,\n",
              "                                 loss='epsilon_insensitive', max_iter=1000,\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
              "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
              "       2.15443469e+02, 1.00000000e+03]),\n",
              "                         'epsilon': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
              "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
              "       3.59381366e+01, 1.00000000e+02])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=2)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'C': np.logspace(-3,3,10), 'epsilon':np.logspace(-2,2,10)}\n",
        "clf = GridSearchCV(estimator = LinearSVR(), param_grid = parameters, n_jobs = -1,\n",
        "                   cv = 10, verbose = 2, scoring = \"neg_mean_absolute_error\")\n",
        "clf.fit(Xpca1, ypca1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuYxp--12rYN",
        "outputId": "ea58b634-6ef7-46e6-87b8-921fe7e752fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.46415888336127775, 'epsilon': 0.01}\n",
            "-0.07491340963374975\n"
          ]
        }
      ],
      "source": [
        "print(clf.best_params_)\n",
        "print(clf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nGDSaA_2rYN"
      },
      "outputs": [],
      "source": [
        "# Usamos KFold de scikit-learn\n",
        "nf = 5  # number of folds\n",
        "kf_r2, kf_rmse, kf_mae = [], [], []  # Listas para ir almacenando los resultados\n",
        "kf = KFold(n_splits=nf, shuffle=True, random_state=1234)\n",
        "\n",
        "for train, test in kf.split(Xpca[2000:20000]):  # pregunta: daría igual poner kf.split(X,y)?\n",
        "    \n",
        "    Xtrain1, ytrain1 = Xpca[train, :], ypca[train].ravel()\n",
        "\n",
        "    # 1. Obtén los datos de test\n",
        "    Xtest1, ytest1 = Xpca[test, :], ypca[test]\n",
        "\n",
        "    # 2. Entrena el modelo (fit) con el conjunto de entrenamiento\n",
        "    svr = LinearSVR(C =  0.46415888336127775, epsilon =  0.01)\n",
        "    svr.fit(Xtrain1, ytrain1)   \n",
        "    \n",
        "    # 3. Predice sobre los datos de test\n",
        "    y_hat = svr.predict(Xtest1)\n",
        "    y_hat = y_hat.reshape(3600, 1)\n",
        "\n",
        "    # Hacemos la inversa de los datos normalizados \n",
        "    y_hat_inverse = scaler2.inverse_transform(y_hat)\n",
        "    ytest_inverse = scaler2.inverse_transform(ytest1)\n",
        "    \n",
        "    # 4. Calcula las métricas r2, rmse y mae\n",
        "    MAE = mae(ytest_inverse, y_hat_inverse)\n",
        "    \n",
        "    # 5. Añádelas a las listas kf_r2, kf_rmse y kf_mae\n",
        "    kf_mae.append(MAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "VJhQ0yML2rYO",
        "outputId": "c946ae99-eb93-40e8-ce97-c6ee9ce054c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.671405649510561\n"
          ]
        }
      ],
      "source": [
        "print(np.mean(kf_mae))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhiTa3UD2rYO"
      },
      "source": [
        "Predecimos sobre los datos finales. Para ello tenemos que aplicar la PCA a X_test_scales (porque el modelo está entrenado para 39 componentes, no para 90 características). Luego, predecir para X_test_scaled_pca. Y lo que obtendremos será y_hat, pero habrá que aplicarle la inversa a la normalización para tener años con sentido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78SIlWCu2rYO",
        "outputId": "bcedff1a-3ece-4fae-e787-b5ab2b20ca67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.69012964 0.4179841  0.5303848  ... 0.40986096 0.50266906 0.32358687]\n",
            " [0.58510291 0.52860714 0.67156418 ... 0.46266765 0.43141059 0.34495695]\n",
            " [0.79299831 0.5435699  0.49853235 ... 0.41657482 0.51168806 0.31806143]\n",
            " ...\n",
            " [0.55466567 0.49606336 0.33750292 ... 0.52259067 0.5481554  0.33009194]\n",
            " [0.54375529 0.295145   0.53587868 ... 0.44992196 0.50930841 0.34569016]\n",
            " [0.65231302 0.507323   0.42179861 ... 0.47794846 0.5020464  0.30612884]]\n",
            "(200000, 90)\n"
          ]
        }
      ],
      "source": [
        "print(X_test_scaled)\n",
        "print(X_test_scaled.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iluPbIJM2rYP"
      },
      "source": [
        "Aplicamos PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6VE6EM_2rYP",
        "outputId": "a7c74f39-87ac-497e-d9b4-19d0b576a2db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200000, 39)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_scaled_pca = pca.transform(X_test_scaled)\n",
        "# Comprobamos qe tiene 39 componentes\n",
        "X_test_scaled_pca.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_CxJnF82rYP"
      },
      "source": [
        "Predecimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OD2Nf_f2rYP",
        "outputId": "217f5c0d-d621-436a-d10e-9e02f27a13c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000,)\n",
            "(200000, 1)\n"
          ]
        }
      ],
      "source": [
        "y_hat = svr.predict(X_test_scaled_pca)\n",
        "print(y_hat.shape)\n",
        "\n",
        "# Reshape\n",
        "y_hat = y_hat.reshape(200000, 1)\n",
        "print(y_hat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svs1e6b92rYQ"
      },
      "source": [
        "Observamos que no son años con sentido las predicciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2WL191u2rYQ",
        "outputId": "037522ba-933d-41e6-ac0a-b20d8b84d7cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.8809593 ],\n",
              "       [0.70549517],\n",
              "       [0.90086073],\n",
              "       ...,\n",
              "       [0.91514191],\n",
              "       [0.89598913],\n",
              "       [0.86228607]])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZuhuBF_d2rYQ",
        "outputId": "6ffead16-b9df-4a93-f0f0-76282119e349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2000.]\n",
            " [1984.]\n",
            " [2001.]\n",
            " ...\n",
            " [2003.]\n",
            " [2001.]\n",
            " [1998.]]\n"
          ]
        }
      ],
      "source": [
        "y_hat = scaler2.inverse_transform(y_hat)\n",
        "# Redondeamos\n",
        "y_hat = np.round(y_hat[:])\n",
        "print(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnmS9PSK2rYQ"
      },
      "outputs": [],
      "source": [
        "#Pasamos a DaraFrame\n",
        "y_hat = pd.DataFrame(y_hat)\n",
        "#Guardamos .csv (lo que hay que subir)\n",
        "y_hat.to_csv('y_hat.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}