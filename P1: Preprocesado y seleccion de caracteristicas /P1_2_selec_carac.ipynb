{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PRÁCTICA 1:  Machine Learning - Parte 2: Métodos filter y wrapper**\n",
        "### Universitat de València, Escola Tecnica Superior d'Enginyeria\n",
        "### Elena Marrero Castellano | 3ª curso del Grado Ciencia de Datos"
      ],
      "metadata": {
        "id": "FK9WNXCYsUWT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LLmvdYr7YOu"
      },
      "source": [
        "# Laboratorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zQEjayv7YOv"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Jj4vtfF7YOw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EimUaThz7YOw"
      },
      "source": [
        "# Selección de caraterísticas mediante métodos *filter*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NYr0hGL7YOw"
      },
      "source": [
        "## Correlación entre las variables de entrada y la de salida\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método consiste en ver qué variables de entrada están más correlacionadas con la salida y escoger aquellas con mayor correlación. Seguimos con el dataset IRIS"
      ],
      "metadata": {
        "id": "S_vGvFod7yNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3p_WVDH7YOw"
      },
      "outputs": [],
      "source": [
        "X, y = load_iris(return_X_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMzXkxpB7YOx"
      },
      "source": [
        "### Ejercicio 1:\n",
        "Calcula la correlación (`np.corrcoef`) entre las entradas y la salidas. Muestra los valores calculados y selecciona dos de las variables más adecuadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwmhkskW7YOx",
        "outputId": "c46ab490-7e70-41bb-b20b-2f732be8e632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.78256123 -0.42665756  0.9490347   0.95654733]\n"
          ]
        }
      ],
      "source": [
        "r = np.zeros(X.shape[1]) # Devuelve una nueva matriz de forma y tipo dados, nuestro caso 4, llena de ceros.\n",
        "for i in range (X.shape[1]):\n",
        "    r[i] = np.corrcoef( X[:,i], y)[0,1]\n",
        "print(r)\n",
        "\n",
        "# Aquí podemos obtener el coeficiente de correlación de las entradas y las salidas:\n",
        "# [ 0.78256123 -0.42665756  0.9490347   0.95654733]\n",
        "# Las variables que he seleccionado son 0.9490347   0.95654733 ya que son las variables que se aproximan más a uno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFSvIkqd7YOy"
      },
      "source": [
        "## Correlación entre variables de entrada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso vamos a descartar variables de entrada que estén muy correlacionadas entre sí, puesto que entendemos que no añaden más información al modelo y pueden perjudicar su rendimiento.\n",
        "\n",
        "Por ejemplo si encontramos que las variables `X[:,0]` y `X[:,1]` (dimensiones 1 y 2) están muy correlacionadas descartaremos una de las dos.\n"
      ],
      "metadata": {
        "id": "b6nkxhU377GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejercicio 2:\n",
        "Calcula la correlación entre todas las variables de entrada y descarta alguna con alta correlación cruzada."
      ],
      "metadata": {
        "id": "_DvQIvgF78oc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD27K_vx7YOy"
      },
      "outputs": [],
      "source": [
        "# Mirando solo la correlación entre todas las variables de entrada obtenemos:\n",
        "correlacion=np.corrcoef((np.array(X[:,0]),\n",
        "                     np.array(X[:,1]),\n",
        "                     np.array(X[:,2]),\n",
        "                     np.array(X[:,3])))\n",
        "\n",
        "# Aquí podemos obtener el coeficiente de correlación de las entradas en forma de matriz:\n",
        "#[[ 1.         -0.11756978  0.87175378  0.81794113]\n",
        "# [-0.11756978  1.         -0.4284401  -0.36612593]\n",
        "# [ 0.87175378 -0.4284401   1.          0.96286543]\n",
        "# [ 0.81794113 -0.36612593  0.96286543  1.        ]]\n",
        "\n",
        "# Las variables que he seleccionado son 0.87175378 y 0.81794113."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOD3-Sh07YOz"
      },
      "source": [
        "#### Ejercicio 3:\n",
        "Representa gráficamente las relaciones entre variables (puedes usar `plt.imshow`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhpY_IpC7YOz",
        "outputId": "d52e13b8-a0d6-41eb-8bef-9c116cf8396d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe9cce1abd0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x1296 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(6,18))\n",
        "plt.imshow(correlacion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUQWwgq67YOz"
      },
      "source": [
        "## Información mutua (MI) entre las variables de entrada y salida"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar vamos a calcular la MI. Por definición sabemos que la MI entre dos variables **discretas** $X,Y$ viene dada por\n",
        "\n",
        "$$ \\textrm{MI}(X,Y) = \\sum_{y\\in \\mathcal{Y}}\\sum_{x\\in \\mathcal{X}} p(x,y)\\log\\left(\\frac{p(x,y)}{p(x)p(y)}\\right)$$\n",
        "\n",
        "Más info: https://en.wikipedia.org/wiki/Mutual_information"
      ],
      "metadata": {
        "id": "djTSGZF-8SJx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8tNbBOV7YO0"
      },
      "source": [
        "En primer lugar estimamos la probabilidad para cada clase de la variable de salida. También podemos calcular su entropía que, viene dada por $ J(y)=-\\sum p(y)\\log p(y) $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnXW4GLh7YO0",
        "outputId": "d176a22a-e338-4a68-9be2-328347468848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p(y) [0.33333333 0.33333333 0.33333333] y entropy -1.0986122886681096\n"
          ]
        }
      ],
      "source": [
        "# Entropia de las variables /discretas/ de salida: J(y) = -sum P(c)log(P(c))\n",
        "classes = np.unique(y)\n",
        "\n",
        "# Probablidad de cada classe\n",
        "py = np.zeros(len(classes))\n",
        "for nc, c in enumerate(classes):\n",
        "    py[c] = np.sum(y == c)\n",
        "    \n",
        "# Convertimos en probabilidades normalizando por el total\n",
        "py /= np.sum(py)\n",
        "\n",
        "# Calculamos la entropía\n",
        "jy = np.sum(py * np.log(py))\n",
        "print('p(y)', py, 'y entropy', jy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlMHZgoI7YO0"
      },
      "source": [
        "### Para poder trabajar con las variables de la entrada tendremos que cuantificarlas dado que son continuas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejercicio 4:\n",
        "Utiliza `KBinsDiscretizer` visto en la primera parte para crear una nueva variable `Xd` discretizando la variable `X`. Utiliza 9 bins."
      ],
      "metadata": {
        "id": "yM5enzJJ8fbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgkGnh-p7YO1",
        "outputId": "1e206608-dfef-47b0-b185-5acee1bda068"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "Xd = KBinsDiscretizer(n_bins=9, encode='ordinal').fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.subplot(1,2,1), plt.hist(Xd,21), plt.title('bins= 9');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpbziMce7YO1"
      },
      "source": [
        "#### Ejercicio 5:\n",
        "Completa el siguiente código para estimar p(x) y p(x,y) y así poder calcular la MI entre cada variable de entrada y la salida.\n",
        "<br>Los trozos de código entre `# --------------` son los que debes rellenar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOksRVEr7YO1",
        "outputId": "843491b8-f63f-4e80-dde9-5d11c7ecc294"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-38-e1c6a1d9dc40>, line 54)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-e1c6a1d9dc40>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    print(mi)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Vamos a calcular la MI para cada variable en X\n",
        "nv = Xd.shape[1]\n",
        "mi = np.zeros(nv)\n",
        "\n",
        "# Valores distintos en Xd\n",
        "xvalues = np.unique(Xd)\n",
        "\n",
        "for v in range(nv):\n",
        "    # Calculamos px\n",
        "    px = np.zeros(len(xvalues))\n",
        "    # (A completar. Fíjate en cómo hemos calculado antes py)\n",
        "    for nc, c in enumerate(xvalues):\n",
        "        \n",
        "        px[nc] = np.sum(Xd == c)\n",
        "    px /= np.sum(px)\n",
        "    \n",
        "    # Calculamos pxy\n",
        "    pxy = np.zeros((len(xvalues), len(classes)))\n",
        "    \n",
        "    # (A completar. Ahora debes tener en cuenta los valores de X e y al mismo tiempo)\n",
        "    for nc, c in enumerate(classes):\n",
        "        for ns, a in enumerate(xvalues):\n",
        "            \n",
        "            pxy[ns,nc] = np.sum((y == c)*(Xd[:,v]== a))\n",
        "                                \n",
        "    pxy /= np.sum(pxy)\n",
        "    \n",
        "    # Calculamos MI: sum_x(sum_y( p(x,y) * log(p(x,y) / (p(x)*p(y)) ))\n",
        "    mi[v] = 0.0\n",
        "    for i in range(pxy.shape[0]):\n",
        "        for j in range(pxy.shape[1]):\n",
        "            # Evitar divisiones por cero y logaritmos = -inf\n",
        "            if pxy[i,j] > 0 and px[i] > 0 and py[j] > 0:\n",
        "                mi[v] +=  sum_x(sum_y( p(x,y) * log(p(x,y) / (p(x)*p(y)) )) \n",
        "\n",
        " print(mi)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qQs_QS77YO2"
      },
      "source": [
        "#### Ejercicio 6:\n",
        "Compara el resultado obtenido con el de la implementación de scikit learn `sklearn.feature_selection.mutual_info_classif`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP-2SPtw7YO2",
        "outputId": "28440044-8b21-45ac-a41c-f53b1c00635a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.50925238, 0.25406388, 0.98580356, 0.98489606])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import feature_selection\n",
        "feature_selection.mutual_info_classif(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXngPwmt7YO2"
      },
      "source": [
        "### En sci-kit learn tenemos una función para seleccionar las mejores `k` variables, `SelectKBest`. Esto junto con la función `mutual_info_classif` nos permite seleccionar grupos de variables."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejercicio 7:\n",
        "Utiliza `sklearn.feature_selection.SelectKBest` y `mutual_info_classif` para seleccionar automáticamente las dos mejores variables."
      ],
      "metadata": {
        "id": "hRSHm10m8yPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ8reCLE7YO2",
        "outputId": "c99e98d5-2d15-4f95-895a-8c4d99fab867"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SelectKBest(k=20,\n",
              "            score_func=array([0.47968604, 0.22772845, 0.98478316, 0.9859296 ]))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "SelectKBest(feature_selection.mutual_info_classif(X,y),k=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hWAQ9uA7YO2"
      },
      "source": [
        "* Hay otros estadísticos que se pueden usar en problemas de clasificación:\n",
        "    - `chi2`: estadístico $\\chi^2$ (chi cuadrado) entre X, y.\n",
        "    - `f_classif`: Test ANOVA entre X, y.\n",
        "- Para problemas de regression podemos usar `mutual_info_regression` o `f_regression`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE_5PK_z7YO3"
      },
      "source": [
        "## Información Mútua entre variables de entrada\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Igual que con la correlación, entre dos variables con MI alto una de ellas podría descartarse."
      ],
      "metadata": {
        "id": "Ub3oZQRl89v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Ejercicio 8:\n",
        "Utiliza `mutual_info_regression` entre cada par de variables de entrada para detectar las que contienen información mútua y descarta una de ellas."
      ],
      "metadata": {
        "id": "CVbmmQZl9Aj4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwaeBX7j7YO3",
        "outputId": "610956ff-30c3-414b-c2d1-f32861078b8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.50033979, 0.23612522, 0.97718943, 0.93249957])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_selection import mutual_info_regression\n",
        "mutual_info_regression(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba2uREo97YO3"
      },
      "source": [
        "# Selección de caraterísticas mediante métodos *wrapper*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los métodos *wrapper* se basan en seleccionar las variables que mejor funcionarán para un determinado modelo. Para ello necesitamos un modelo de clasificación o regresión, e iremos probando todas las combinaciones de variables una a una y en grupos.\n",
        "\n",
        "Vamos a experimentar con este método con un problema sencillo de regresión donde ajustaremos una recta por mínimos cuadrados. El problema lo generaremos con la función `make_regression` de scikit-learn, asegurándonos de crear un número de variables informativas menor que el número total de variables, tal y como muestra el código a continuación.\n",
        "\n",
        "Generamos 100 muestras de un problema de regresión con 4 variables de entrada y una de salida. De las 4 variables solo 3 son necesarias para poder predecir. Además añádimos una pequeña cantidad de ruido para complicar un poco el problema."
      ],
      "metadata": {
        "id": "tpNpg6Fc7mUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X9p36LC7YO3"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "# Para que el ejemplo tenga sentido tenemos que poner n_informative < n_features\n",
        "X, y = make_regression(n_samples=100, n_features=4, n_informative=3, n_targets=1, noise=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eW4tiUJ7YO3"
      },
      "source": [
        "### En la siguiente celda de código hacemos dos cosas:\n",
        "1. Dividimos el conjunto de `X,y` en entrenamiento (train) y test.\n",
        "2. Definimos una clase de Python con un modelo sencillo de regresión lineal.\n",
        "\n",
        "Como os podéis imaginar ya existen modelos así en scikit-learn. El objetivo de definir uno nosotros es doble: 1) para que tengáis un ejemplo de cómo se hace, y 2) para que os familiarizéis con la programación de objetos en Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFs2aLlz7YO4",
        "outputId": "f3a13692-b3ec-4429-9886-0bfc233738a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE 0.00015988410589338776\n",
            "[[ 7.35436984e+01  7.27509412e+00 -1.64508003e-03  1.58943126e+01\n",
            "   2.20625961e-03]]\n"
          ]
        }
      ],
      "source": [
        "# Separamos en entrenamiento y test\n",
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# Nos aseguramos de que la variable y tenga dimensión Nx1\n",
        "ytrain = ytrain.reshape(-1,1)\n",
        "ytest = ytest.reshape(-1,1)\n",
        "\n",
        "# Definimos un modelo sencillo de regresión lineal\n",
        "class myLinearModel:\n",
        "    def __init__(self):\n",
        "        self.alphas = None\n",
        "    \n",
        "    def add_ones(self, X):\n",
        "        ones = np.ones((X.shape[0] ,1))\n",
        "        return np.hstack((X, ones))\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        # y = X*alphas => pinv(X)*y = alphas\n",
        "        self.alphas = np.dot(np.linalg.pinv(self.add_ones(X)), y)\n",
        "        return self.alphas\n",
        "\n",
        "    # Con esta función predecimos\n",
        "    def predict(self, X):\n",
        "        if self.alphas is not None:\n",
        "            return np.dot(self.add_ones(X), self.alphas)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Y con esta otra medimos el error\n",
        "    def score(self, y, yp):\n",
        "        print('MSE', np.sum((y - yp)**2) / len(y))\n",
        "\n",
        "# Esto ya existe en sklearn, pero mola más programarlo un mismo!\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# lr = LinearRegression()\n",
        "# lr.fit(Xtrain, ytrain)\n",
        "# yp = lr.predict(Xtest)\n",
        "# print(lr.score(Xtest, ytest))\n",
        "\n",
        "# Comprobamos que funciona\n",
        "model = myLinearModel()\n",
        "model.fit(Xtrain, ytrain)\n",
        "yp = model.predict(Xtest)\n",
        "model.score(ytest, yp)\n",
        "print(model.alphas.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVLKZnGE7YO4"
      },
      "source": [
        "### Vamos ahora con el método de selección de variables propiamente dicho.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### En primer lugar vamos probando con cada variable una por una ...\n"
      ],
      "metadata": {
        "id": "bUaUHh299U9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ejercicio 9:\n",
        "Prueba una por una las variables de entrada en nuestro `myLinearModel`. Comprueba los resultados\n",
        "con las funciones `fit`, `predict` y `score`."
      ],
      "metadata": {
        "id": "72g48s4G9V_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz_dac7u7YO4",
        "outputId": "6c991571-eed0-40c5-ff68-5775bd76fa5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE 345.29994679208437\n",
            "[[73.9281123   2.53725325]]\n",
            "MSE 5378.450901375113\n",
            "[[-3.76347308  1.72393899]]\n",
            "MSE 4947.283621286139\n",
            "[[-22.78018258   7.66689261]]\n",
            "MSE 4783.2450689660745\n",
            "[[24.348452 -1.44923 ]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(Xtrain.shape[1]):\n",
        "    # Cada variable esta en Xtrain[:, i, None]\n",
        "    # Usa fit, predict y score para comprobar el resultado\n",
        "    model = myLinearModel()\n",
        "    model.fit(Xtrain[:,i,None],ytrain)\n",
        "    yp = model.predict(Xtrain[:,i,None]) \n",
        "    model.score(ytrain, yp)\n",
        "    print(model.alphas.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpeg6fBn7YO5"
      },
      "source": [
        "#### Ahora en grupos de dos ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ejercicio 10:\n",
        "Ahora prueba todas las combinaciones de dos variables. Para ello usaremos `combinations` de la librería `itertools`."
      ],
      "metadata": {
        "id": "llSLStvG9eAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RannxFgr7YO5",
        "outputId": "a22f76f9-518e-448f-fb75-e7b689f2df87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE 247.70782288410592\n",
            "[[75.89523127 10.3779414   1.58192552]]\n",
            "MSE 344.64983113957516\n",
            "[[74.19934652  0.91561113  2.28830388]]\n",
            "MSE 46.32758241289291\n",
            "[[72.02459367 17.16206467  0.52263982]]\n",
            "MSE 4941.705202667163\n",
            "[[ -2.44155218 -22.62372878   7.85570428]]\n",
            "MSE 4723.900091481465\n",
            "[[-8.05279579 25.58397449 -0.82662601]]\n",
            "MSE 4324.119930340383\n",
            "[[-23.15489051  24.64225726   4.92113145]]\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "for c in combinations(range(4), 2):\n",
        "    # Cada combo está en Xtrain[:,c]\n",
        "    model = myLinearModel()\n",
        "    model.fit(Xtrain[:,c],ytrain)\n",
        "    yp = model.predict(Xtrain[:,c])\n",
        "    model.score(ytrain, yp)\n",
        "    print(model.alphas.T)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBBs1Btx7YO5"
      },
      "source": [
        "#### Y por último en grupos de tres ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY2X2YtS7YO5",
        "outputId": "8cf4f14e-138e-4e84-961c-aa6d9f03b190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE 247.11415104613224\n",
            "[[76.15386943 10.37500798  0.87496731  1.344297  ]]\n",
            "MSE 0.0001159557299748622\n",
            "[[7.35441953e+01 7.27510261e+00 1.58942408e+01 1.76661330e-03]]\n",
            "MSE 46.3260752036175\n",
            "[[ 7.20113091e+01 -4.41389347e-02  1.71639502e+01  5.34419601e-01]]\n",
            "MSE 4282.711043460083\n",
            "[[ -6.7387271  -22.73871648  25.67088442   5.3276403 ]]\n"
          ]
        }
      ],
      "source": [
        "from itertools import combinations\n",
        "for c in combinations(range(4), 3):\n",
        "    # Cada combo está en Xtrain[:,c]\n",
        "    model = myLinearModel()\n",
        "    model.fit(Xtrain[:,c],ytrain)\n",
        "    yp = model.predict(Xtrain[:,c])\n",
        "    model.score(ytrain, yp)\n",
        "    print(model.alphas.T)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0NYr0hGL7YOw",
        "_DvQIvgF78oc",
        "iOD3-Sh07YOz",
        "dlMHZgoI7YO0",
        "yM5enzJJ8fbG",
        "JpbziMce7YO1",
        "lXngPwmt7YO2",
        "PE_5PK_z7YO3",
        "CVbmmQZl9Aj4",
        "5eW4tiUJ7YO3",
        "bUaUHh299U9m",
        "vBBs1Btx7YO5"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}