{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PRÁCTICA 7:  Machine Learning - Proyecto final**\n",
        "### Universitat de València, Escola Tecnica Superior d'Enginyeria\n",
        "### Elena Marrero Castellano | 3ª curso del Grado Ciencia de Datos"
      ],
      "metadata": {
        "id": "FK9WNXCYsUWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desarrolla un algoritmo que a partir de unos datos de entrada (X) prediga los valores de salida (Y). La nota no va a depender únicamente de los resultados obtenidos sino de el procedimiento utilizado. Esmérate en incluir comentarios que describan el procedimiento y la motivación de cada paso. Se valorará el uso de todos los conocimientos desarrollados durante las prácticas. Entre otros:\n",
        "\n",
        "- Métodos de preprocesado.\n",
        "- Métodos de selección de características.\n",
        "- Métodos de extracción de características.\n",
        "- Métodos de partición de datos durante el entrenamiento.\n",
        "- Métodos de clasificación/regresión, cuantos más se prueben mejor.\n",
        "\n",
        "Entregaréis:\n",
        "- Un fichero 'y_hat.cvs' con vuestra predicciones sobre los datos de test.\n",
        "- Un fichero '.ipynb' que contenga el procedimiento utilizado debidamente comentado."
      ],
      "metadata": {
        "id": "3NGwAh94BZE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comenzamos\n",
        "\n",
        "Nuestro objetivo va a ser entrenar un modelo para ver si luego funciona correctamente. Antes de empezar tenemos que descargar las librerias necesarias para la realización de la práctica"
      ],
      "metadata": {
        "id": "JyqEHIoUBa72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8_F-Zn6BVF9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.stats import mode\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer, PowerTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "import sklearn.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHuufCIBVF_"
      },
      "source": [
        "## Adquisición de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiE8UedqBVF_"
      },
      "source": [
        "Empezamos como siempre descargando nuestros datos y a partir de los mismos dividimos el conjunto inicial de los datos en dos. Normalmente deberemos coger 2/3 del conjunto de datos inicial para entrenar (siendo Xtrain, ytrain los nombres de dichas variables), por otro lado el 1/3 restante para probar los datos y ver si funcionan correctamete (siendo Xtest, ytest los nombres de dichas variables). Con las variables Xtest, ytest no se trabaja con ellas en entrenamiento, asegurandonos así que no conozcan los datos reales para poderlos tener en cuenta a la hora de definir como de bueno es el modelo. Pero en este caso no vamos a dicidir el conjunto de datos de esta forma ya que tenemos ya la adquisición de datos dada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOMurSNbBVGA",
        "outputId": "6162163f-6bd7-4c98-8573-74fa4a3ddb48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['AGE', 'SEX', 'BMI', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y',\n",
            "       'C'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Descargamos \n",
        "# Read data and start to work\n",
        "df_train = pd.read_table('diabetes_train.data')\n",
        "df_test = pd.read_table('diabetes_test.data')\n",
        "\n",
        "# Nombres de las variables\n",
        "print(df_train.keys())\n",
        "data_train = df_train.to_numpy()\n",
        "\n",
        "ytrain, ytrain_class = data_train[:,-2], data_train[:,-1]\n",
        "Xtrain = data_train[:, :-2]\n",
        "\n",
        "# Test set\n",
        "Xtest = df_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hefmt0B8BVGB"
      },
      "source": [
        "## Preprocesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z0LY0LrBVGB"
      },
      "source": [
        "Comenzamos realizando una normalización de nuestros datos porque mchos modelos pueden sensibles a los valores de las características implicadas o para entender esa relación entre las variables. Todas las muestras deben normalizarse con los parámetros correspondientes al conjunto de entrenamiento. Nosotros en este curso hemos visto varias de ellas, la normalizacion, la normalización quitando el mínimo, la estandarización o usando los modelos de normalización (MinMaxScaler, MaxAbsScaler o StandardScaler). En este caso voy a realizarlo con el MinMaxScaler ya tiene un parámetro que nos dice el rango el cual queremos meter los datos (feature_range), por defecto sería entre 0 y 1. Este método, como sabemos se utiliza el escalado entre los rangos que yo los busco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWh0MZqCBVGB"
      },
      "outputs": [],
      "source": [
        "# Normalizamos\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Ajuste solo en el set de entrenamiento.\n",
        "scaler.fit(Xtrain)\n",
        "\n",
        "# Aplicamos la transformación tanto al conjunto de train como al conjunto de test.\n",
        "X_train_scaled = scaler.transform(Xtrain)\n",
        "X_test_scaled = scaler.transform(Xtest)\n",
        "\n",
        "# Podemos transformar estos datos a escalado o trabajar directamente con ellos, vamos a probar las dos formas \n",
        "# y_train_scaled = scaler2.transform(ytrain)\n",
        "y_train = ytrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx-ZejvtBVGC"
      },
      "source": [
        "Además de la normalización, en ocasiones se requiere una etapa adicional de codificación porque los modelos pueden necesitar que las variables presenten un formato específico: (numérico continuo, numérico discreto, etc.) Pero en este caso no vamos a realizar ninguna codicación porque no es necesaria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me_iQnotBVGC"
      },
      "source": [
        "Nuestro segundo paso en el preprocesado de los datos sería realizar una **selección de características y/o una extracción de características**. Los métodos de **selección** escogen algunas de las variables originales del prolema, estas son más adecuadas cuando no se quiere  comprometer la interpretabilidad de los datos. Sin embargo, si lo que buscamos es eficiencia, los **métodos de extracción** son mejores ya que reducen la dimensionalidad pero preservando la mayor parte de la información de todas las variables originales. En este caso, lo que voy a realizar es para cada caso una cosa, para los datos de clasificación podríamos realizar una selección de características pero como primero vamos a trabajar con los datos de regresión trabajaremos directamente una extracción de características la cual podemos utilizar de cualquier tipo de los vistos en clase (PCA, LLE), en este caso la realización de la PCA no nos sirve porque al tener pocos datos no realiza su función ya que es una técnica que se utiliza para enfatizar la variación y resaltar patrones sólidos en un conjunto de datos. A menudo se utiliza para que los datos sean fáciles de explorar y visualizar.\n",
        "\n",
        "Igualemte la he realizado para comprobar que lo que digo es cierto, si esto es así y realmente la PCA no es necesaria lo veremos reflejados en los modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnizObt4BVGC"
      },
      "source": [
        "Para aplicar la PCA, necesitaremos saber cuantas componenct elegir, por lo tanto con nuestras variables normalizadas y sin normalizar vamos a buscar cual sería la instancia del modelo que vamos a utilizar. Significando PCA (.90), por ejemplo que scikit-learn elige el número mínimo de componentes principales de manera que se retiene el 90% de la varianza. Es lo mismo que utilizamos en las prácticas con explained_variance_ (la cantidad de variación explicada por cada uno de los componentes seleccionados) si no que al revés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plk51Ok7BVGC",
        "outputId": "3c9bc5b3-1e46-49ef-ac27-ca3f24165057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En este caso, el 100% de la variación equivale a 1  componentes principales\n",
            "En este caso, el 95% de la variación equivale a 7  componentes principales\n",
            "En este caso, el 90% de la variación equivale a 6  componentes principales\n",
            "En este caso, el 85% de la variación equivale a 5  componentes principales\n",
            "En este caso, el 80% de la variación equivale a 4  componentes principales\n"
          ]
        }
      ],
      "source": [
        "# Nuestros valores están normalizados, por lo que ahora aplicamos PCA\n",
        "# Crea una instancia del modelo\n",
        "pca_100 = PCA(.100)\n",
        "pca_95 = PCA(.95)\n",
        "pca_90 = PCA(.90)\n",
        "pca_85 = PCA(.85)\n",
        "pca_80 = PCA(.80)\n",
        "\n",
        "# Ajustamos \n",
        "pca_100.fit(X_train_scaled) \n",
        "pca_95.fit(X_train_scaled) \n",
        "pca_90.fit(X_train_scaled) \n",
        "pca_85.fit(X_train_scaled) \n",
        "pca_80.fit(X_train_scaled) \n",
        "\n",
        "# Imprime el número de componentes generados\n",
        "print(\"En este caso, el 100% de la variación equivale a\", pca_100.n_components_,\" componentes principales\")\n",
        "print(\"En este caso, el 95% de la variación equivale a\", pca_95.n_components_,\" componentes principales\")\n",
        "print(\"En este caso, el 90% de la variación equivale a\", pca_90.n_components_,\" componentes principales\")\n",
        "print(\"En este caso, el 85% de la variación equivale a\", pca_85.n_components_,\" componentes principales\")\n",
        "print(\"En este caso, el 80% de la variación equivale a\", pca_80.n_components_,\" componentes principales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDVqo5D9BVGD",
        "outputId": "c8b5536b-236e-49fb-84b4-a6c9f41963ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape Xpca:  (309, 7)\n",
            "Shape ypca:  (309,)\n"
          ]
        }
      ],
      "source": [
        "# Eligiendo como lo que creo que sería lo mas correcto como xxx componentes principales:\n",
        "model_pca = PCA(n_components=7)\n",
        "model_pca.fit(X_train_scaled) \n",
        "Xpca = model_pca.transform(X_train_scaled)\n",
        "ypca = y_train\n",
        "\n",
        "# Para ver si se ha hecho bien vamos a mirar los shape de cada uno \n",
        "print(\"Shape Xpca: \", Xpca.shape)\n",
        "print(\"Shape ypca: \", ypca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp1opm52BVGD"
      },
      "source": [
        "## Desarrollo del modelo \n",
        "\n",
        "Una vez realizada el preprocesado de los datos vamos a pasar a la realización de nuestro modelo. Vamos a realizarlo mediante las SVM y los árboles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUCCU28dBVGD"
      },
      "source": [
        "El apartado de métodos de partición de datos durante el entrenamiento, he decidido no realizarlo ya que tenemos muy pocos datos y no vale la pena realizar estos pasos, si hubiera sido un data set más grande obviamente hubiera sido mejor "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdxplgTnBVGE"
      },
      "source": [
        "#### SVR, con PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMeq0Y73BVGE",
        "outputId": "418274e1-a17d-4b74-b894-fef247075eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0,\n",
              "                                 fit_intercept=True, intercept_scaling=1.0,\n",
              "                                 loss='epsilon_insensitive', max_iter=1000,\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
              "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
              "       2.15443469e+02, 1.00000000e+03]),\n",
              "                         'epsilon': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
              "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
              "       3.59381366e+01, 1.00000000e+02])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=2)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'C': np.logspace(-3,3,10), 'epsilon':np.logspace(-2,2,10)}\n",
        "\n",
        "clf_svr1 = GridSearchCV(estimator = sklearn.svm.LinearSVR(), param_grid = parameters, n_jobs = -1,cv = 5, verbose = 2, scoring = \"neg_mean_absolute_error\")\n",
        "clf_svr1.fit(Xpca, ypca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhqlk8wSBVGE"
      },
      "source": [
        "#### RandomForestRegressor, con PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsY9pls2BVGE",
        "outputId": "7f4abb67-0526-42da-80aa-206f2de2cf9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 84 candidates, totalling 840 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    6.5s\n",
            "[Parallel(n_jobs=-1)]: Done 682 tasks      | elapsed:   16.6s\n",
            "[Parallel(n_jobs=-1)]: Done 840 out of 840 | elapsed:   20.9s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
              "                                             criterion='mse', max_depth=None,\n",
              "                                             max_features='sqrt',\n",
              "                                             max_leaf_nodes=None,\n",
              "                                             max_samples=None,\n",
              "                                             min_impurity_decrease=0.0,\n",
              "                                             min_impurity_split=None,\n",
              "                                             min_samples_leaf=1,\n",
              "                                             min_samples_split=2,\n",
              "                                             min_weight_fraction_leaf=0.0,\n",
              "                                             n_estimators=100, n_jobs=None,\n",
              "                                             oob_score=False, random_state=None,\n",
              "                                             verbose=0, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'max_depth': range(1, 8),\n",
              "                         'n_estimators': range(68, 128, 5)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=2)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'n_estimators': range(68,128,5), 'max_depth': range(1,8)}\n",
        "\n",
        "clf_rf1 = GridSearchCV(estimator = RandomForestRegressor(max_features = \"sqrt\"), param_grid = parameters, n_jobs = -1, cv = 5, verbose = 2, scoring = \"neg_mean_absolute_error\")\n",
        "clf_rf1.fit(Xpca, ypca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At6pwuvDBVGE"
      },
      "source": [
        "Pero, ¿Qué pasaría si utilizasemos los datos sin realizar la PCA? Volvamos a realizar el mismo proceso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_gGGLG6BVGF"
      },
      "source": [
        "#### SVR, sin PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpAO_P04BVGF",
        "outputId": "93e61e1d-a180-4168-e2d4-44fa1de93d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    1.3s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LinearSVR(C=1.0, dual=True, epsilon=0.0,\n",
              "                                 fit_intercept=True, intercept_scaling=1.0,\n",
              "                                 loss='epsilon_insensitive', max_iter=1000,\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
              "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
              "       2.15443469e+02, 1.00000000e+03]),\n",
              "                         'epsilon': array([1.00000000e-02, 2.78255940e-02, 7.74263683e-02, 2.15443469e-01,\n",
              "       5.99484250e-01, 1.66810054e+00, 4.64158883e+00, 1.29154967e+01,\n",
              "       3.59381366e+01, 1.00000000e+02])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=2)"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'C': np.logspace(-3,3,10), 'epsilon': np.logspace(-2,2,10)}\n",
        "\n",
        "svr = GridSearchCV(estimator = sklearn.svm.LinearSVR(), param_grid = parameters, n_jobs = -1, cv = 5, verbose = 2, scoring = \"neg_mean_absolute_error\")\n",
        "svr.fit(X_train_scaled, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ush4PC1BVGF"
      },
      "source": [
        "#### DecisionTreeRegresor sin PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44A5N-tVBVGF",
        "outputId": "7807d8e3-9805-47de-cb3e-7d96adce88a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse',\n",
              "                                             max_depth=None, max_features=None,\n",
              "                                             max_leaf_nodes=None,\n",
              "                                             min_impurity_decrease=0.0,\n",
              "                                             min_impurity_split=None,\n",
              "                                             min_samples_leaf=1,\n",
              "                                             min_samples_split=2,\n",
              "                                             min_weight_fraction_leaf=0.0,\n",
              "                                             presort='deprecated',\n",
              "                                             random_state=None,\n",
              "                                             splitter='best'),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid=[{'max_depth': range(1, 10)}], pre_dispatch='2*n_jobs',\n",
              "             refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=1)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param_grid = [{'max_depth': range(1,10)}]\n",
        "dtr = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid, cv=10, verbose=1, n_jobs=-1, scoring = \"neg_mean_absolute_error\")\n",
        "dtr.fit(X_train_scaled, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oWfGVZNBVGG"
      },
      "source": [
        "#### RandomForestRegressor, sin PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu1zx7gBBVGG",
        "outputId": "dc645473-ee26-4d7d-cad2-1d4a0cba8aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 98 candidates, totalling 490 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=-1)]: Done 490 out of 490 | elapsed:    5.0s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
              "                                             criterion='mse', max_depth=None,\n",
              "                                             max_features='sqrt',\n",
              "                                             max_leaf_nodes=None,\n",
              "                                             max_samples=None,\n",
              "                                             min_impurity_decrease=0.0,\n",
              "                                             min_impurity_split=None,\n",
              "                                             min_samples_leaf=1,\n",
              "                                             min_samples_split=2,\n",
              "                                             min_weight_fraction_leaf=0.0,\n",
              "                                             n_estimators=100, n_jobs=None,\n",
              "                                             oob_score=False, random_state=None,\n",
              "                                             verbose=0, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid=[{'max_depth': range(1, 8),\n",
              "                          'n_estimators': range(0, 70, 5)}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=1)"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "rfr = GridSearchCV(RandomForestRegressor(max_features = \"sqrt\"),\n",
        "                  param_grid = param_grid,\n",
        "                  cv = 5,\n",
        "                  verbose = 1,\n",
        "                  n_jobs = -1)\n",
        "\n",
        "rfr.fit(X_train_scaled, ytrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoUNb2-VBVGG"
      },
      "source": [
        "#### SVC, sin PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqPPJxy2BVGG",
        "outputId": "06da8294-d435-42ff-9824-f7512f15c2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
            "/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'C': array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
              "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
              "       2.15443469e+02, 1.00000000e+03])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=2)"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'C': np.logspace(-3,3,10)}\n",
        "\n",
        "svc = GridSearchCV(estimator = sklearn.svm.LinearSVC(), param_grid = parameters, n_jobs = -1, cv = 5, verbose = 2, scoring = \"neg_mean_absolute_error\")\n",
        "svc.fit(X_train_scaled, ytrain_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woyof3_OBVGH"
      },
      "source": [
        "#### DecisionTreeClassifier, sin PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da4tXJ4JBVGH",
        "outputId": "c1dd9f8d-873e-46bb-e95c-1bfc42ad28fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features=None,\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              presort='deprecated',\n",
              "                                              random_state=None,\n",
              "                                              splitter='best'),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid=[{'max_depth': range(1, 10)}], pre_dispatch='2*n_jobs',\n",
              "             refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=1)"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "param_grid = [{'max_depth': range(1,10)}]\n",
        "dtc = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, cv=10, verbose=1, n_jobs=-1, scoring = \"neg_mean_absolute_error\")\n",
        "dtc.fit(X_train_scaled,ytrain_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BIif1uMBVGH"
      },
      "source": [
        "#### RandomForestClassifier, sin PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2qPbN2gBVGH",
        "outputId": "6f5d7d1f-51b5-4e66-d918-7aa5fe8464f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:    7.0s\n",
            "[Parallel(n_jobs=-1)]: Done 315 out of 315 | elapsed:    7.9s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                                              class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='sqrt',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              max_samples=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=100, n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'max_depth': range(1, 10),\n",
              "                         'n_estimators': range(68, 100, 5)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='neg_mean_absolute_error', verbose=2)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'n_estimators': range(68,100,5), 'max_depth': range(1,10)}\n",
        "\n",
        "rfc = GridSearchCV(estimator = RandomForestClassifier(max_features = \"sqrt\"), \n",
        "                                 param_grid = parameters, \n",
        "                                 n_jobs = -1, cv = 5, \n",
        "                                 verbose = 2, \n",
        "                                 scoring = \"neg_mean_absolute_error\")\n",
        "rfc.fit(X_train_scaled,ytrain_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFAIbESkBVGH"
      },
      "source": [
        "## Validación del modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGZInyB0BVGI",
        "outputId": "6e262ba9-2ac8-4906-92cc-38976dc227a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   \n",
            "----------------------------------------------------------------------------\n",
            "best_score_svr sin PCA:  {'C': 46.41588833612773, 'epsilon': 0.01}\n",
            "best_score_dtr sin PCA:  {'max_depth': 3}\n",
            "best_score_rfr sin PCA:  {'max_depth': 5, 'n_estimators': 55}\n",
            "best_score_svc sin PCA:  {'C': 46.41588833612773}\n",
            "best_score_dtc sin PCA:  {'max_depth': 3}\n",
            "best_score_rfc sin PCA:  {'max_depth': 4, 'n_estimators': 83}\n",
            "----------------------------------------------------------------------------\n",
            "best_score_svr sin PCA:  -43.917863517355364\n",
            "best_score_dtr sin PCA:  -50.1365626481907\n",
            "best_score_rfr sin PCA:  0.44270328012330956\n",
            "best_score_svc sin PCA:  -0.45282919090428353\n",
            "best_score_dtc sin PCA:  -0.4825806451612903\n",
            "best_score_rfc sin PCA:  -0.41443680592279214\n",
            "----------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"   \")\n",
        "print(\"----------------------------------------------------------------------------\")\n",
        "print(\"best_score_svr sin PCA: \", svr.best_params_)\n",
        "print(\"best_score_dtr sin PCA: \", dtr.best_params_)\n",
        "print(\"best_score_rfr sin PCA: \", rfr.best_params_)\n",
        "print(\"best_score_svc sin PCA: \", svc.best_params_)\n",
        "print(\"best_score_dtc sin PCA: \", dtc.best_params_)\n",
        "print(\"best_score_rfc sin PCA: \", rfc.best_params_)\n",
        "print(\"----------------------------------------------------------------------------\")\n",
        "print(\"best_score_svr sin PCA: \", svr.best_score_)\n",
        "print(\"best_score_dtr sin PCA: \", dtr.best_score_)\n",
        "print(\"best_score_rfr sin PCA: \", rfr.best_score_)\n",
        "print(\"best_score_svc sin PCA: \", svc.best_score_)\n",
        "print(\"best_score_dtc sin PCA: \", dtc.best_score_)\n",
        "print(\"best_score_rfc sin PCA: \", rfc.best_score_)\n",
        "print(\"----------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVYDLPYqBVGI"
      },
      "source": [
        "## Obtención de conclusiones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpD4hSxwBVGI"
      },
      "outputs": [],
      "source": [
        "# Inicializamos el predictor para la clasificacion\n",
        "predictor = DecisionTreeClassifier\n",
        "criterion = 'entropy'\n",
        "max_depth = 3\n",
        "report = sklearn.metrics.classification_report\n",
        "\n",
        "# predictor\n",
        "dt = predictor(criterion=criterion, max_depth=max_depth)\n",
        "dt.fit(Xtrain, ytrain_class)\n",
        "\n",
        "y_hat = dt.predict(Xtest)\n",
        "y_hat = np.round(y_hat)\n",
        "\n",
        "y_hat = pd.DataFrame(y_hat)\n",
        "y_hat.to_csv('yhat_clasificacion.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2OZOY9HBVGI"
      },
      "outputs": [],
      "source": [
        "# Inicializamos el predictor para la regresion\n",
        "predictor = RandomForestRegressor\n",
        "\n",
        "# predictor\n",
        "dt = predictor()\n",
        "dt.fit(Xtrain, ytrain)\n",
        "\n",
        "y_hat = dt.predict(Xtest)\n",
        "y_hat = np.round(y_hat)\n",
        "\n",
        "y_hat = pd.DataFrame(y_hat)\n",
        "y_hat.to_csv('yhat_regresion.csv')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}